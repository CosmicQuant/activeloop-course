{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10H9AWkbXf5KpB8V5l7uSD3vv95Tf0sDs",
      "authorship_tag": "ABX9TyNWBziZ904r8pMJI270E6JJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CosmicQuant/activeloop-course/blob/main/Exploring_the_World_of_Language_Models_LLMs_vs_Chat_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DLAXR51s1W8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken python-dotenv pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/drive/MyDrive/keys.env')"
      ],
      "metadata": {
        "id": "cZeHDxojva4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "  input_variables=[\"product\"],\n",
        "  template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print( chain.run(\"wireless headphones\") )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApHfBpD5uu9p",
        "outputId": "25a079f9-3af8-4888-ddb2-c2c18d9213f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Wireless Audio Solutions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "  HumanMessage,\n",
        "  SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "\tSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "\tHumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmlzb7CYx2Oc",
        "outputId": "274faa49-5b1b-4dd9-98a3-f8c5afb633f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "  [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "  ],\n",
        "  [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
        "  ],\n",
        "]\n",
        "print( chat.generate(batch_messages) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYTJkI5Cz46M",
        "outputId": "7cf98ae1-397f-46f2-dfae-c4ae816aef6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text='I like programming.', generation_info=None, message=AIMessage(content='I like programming.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 65, 'completion_tokens': 12, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'} run=RunInfo(run_id=UUID('e6901177-0519-4bb8-9c68-f81ce8d21e13'))\n"
          ]
        }
      ]
    }
  ]
}